# Two-Stage Radio Map Estimation

This repository builds on [previous work](https://github.com/GeoAICenter/radio-map-estimation-pimrc2023) in radio map estimation (RME), predicting radio power across a two-dimensional area. As in previous experiments, we don't have knowledge of transmitter location(s), and the environment contains buildings which block and reflect radio signals. We take as input a small number of radio power measurements and a mask showing the locations of these sampled measurements and the buildings in the environment. The model then outputs a complete radio map showing predicted radio power at all locations in the 2D area (optionally including predictions at building locations).

![Image](images/example_input_output.png)

*Sample Input and Output: Sampled Map and Environment Mask are fed as input to the model, which outputs Complete Radio Map. Input and ground truth maps are adapted from the RadioMapSeer Dataset, discussed below.*

The idea of two-stage radio map estimation is to split the task of RME into two parts: first, taking the sparsely sampled measurements and predicting a dense radio map *without reference to buildings*, as if the signal were propagating through empty space; second, taking this dense radio map prediction and convolving it with a map of building locations ("Building Mask") to produce a more accurate radio map that takes the physical environment into account.

![Image](images/example_input_output_2.png)

*Sample Input and Output of Two-Stage RME: Instead of jumping straight from the sampled map to the complete radio map with buildings, an intermediate map is produced ("Dense Map") that ignores buildings and just extrapolates from sample strength and location.*

The intuition behind this is that radio propagation has unique physical characteristics, in particular a source of propagation (i.e. a transmitter) and a relationship between signal strength and distance from that source, that most image data don't have. A model that can learn these physical characteristics will presumably perform better than one that treats the radio map like a normal image. The first stage of the model is designed to learn these spatial relationships, and the second stage to combine this with environmental information. Our main model, MAE_CBAM, uses a masked vision transformer with learnable position encodings, self-attention, and cross-attention to accomplish the first goal, and a CNN / UNet with CBAM-style gated attention for the second. Other models replace / remove different elements of this main model to carry out ablation studies.

## Model Architectures

The two-stage RME models are composed of two sub-models, one for each stage. As stated above, for the main [MAE_CBAM](models/mae_cbam.py) model, these are a masked vision transformer and a UNet with CBAM gated attention. The masked vision transformer is adapted from the code for Masked Autoencoders Are Scalable Vision Learners ([paper](https://arxiv.org/abs/2111.06377) | [github](https://github.com/facebookresearch/mae)), and the UNet is adapted from our previous code with an added implemention of CBAM: Convolutional Block Attention Module ([paper](https://arxiv.org/abs/1807.06521) | [github](https://github.com/xmu-xiaoma666/External-Attention-pytorch/blob/master/README_EN.md#6-cbam-attention-usage)).

For ablation tests, we also have an [MAE_UNet](models/mae_unet.py) model, which removes CBAM attention from the UNet in the second stage of the model; the [Interpolation_CBAM](models/interpolation_cbam.py) and [Interpolation_UNet](models/interpolation_unet.py) models, which replace the masked vision transformer with basic interpolation; and the [CBAM](models/cbam.py) and [UNet](models/unet.py) models which remove the first stage altogether and feed the sparsely sampled map directly into a UNet with or without CBAM attention. The UNet model here is similar but not identical to the UNet model in our previous work.

*Diagram of all two-stage models (MAE_CBAM, MAE_UNet, Interpolation_CBAM, Interpolation_UNet). Single-stage models (CBAM, UNet) omit the first stage.*

**Important implementation note:** while the above diagram shows a sampled map being fed into the model as input, for training purposes all models actually taken in the *complete* radio map as input and carry out sampling within stage 1 of the model (so the model does not "see" the complete map when making its predictions, but carries out sampling as a pre-processing step within the model). For inference or evaluation, the user can set the hyperparameter *pre_sampled=True* to feed already sampled maps directly to the model, but (1) such maps have to have a batch size of 1, and (2) the sampled maps must have an accompanying mask that specifies sampled locations (if *pre_sampled=False*, the model generates this mask while sampling the complete map). The reason for the batch size of 1, and why training the models on pre-sampled maps isn't an option, is due to limitations in the training set, and could be changed in the future. This will be discussed in more detail in the sections on the MAE sub-model and the dataset below.

## Sub-Model Architectures

The sub-models that make up stages 1 and 2 of the two-stage models are saved in the sub_models folder, and are all preceded with an underscore "_" to distinguish them from similarly named full models. The four sub-models currently implemented are discussed below, with links to relevant papers and github repositories where they have been copied or adapted from other projects.

**MAE** ([paper](https://arxiv.org/abs/2111.06377) | [github](https://github.com/facebookresearch/mae))

The masked autoencoder (MAE) is a masked vision transformer with similar principles to masked language models, e.g. BERT. It takes a complete image, masks parts of it, and then attempts to predict the missing parts. More specifically, it starts by taking a complete image, splitting it into patches, masking some of those patches, and then passing the *unmasked patches only* through a transformer encoder with self-attention to create an embedding of each visible patch. In our case, the visible patches correspond to sampled measurements on the radio map, and we set *patch_size=1* to mask and embed the image at a pixel level. In the second part of the MAE, the masked pixels (given a learnable "MASK" embedding) are reattached to the embeddings of the unmasked pixels, and passed through a smaller decoder with self-attention to create embeddings for all pixels in the image, both masked and unmasked, which are then used to predict the RGB values or signal strength of the original image at all locations.

Our implementation makes a few changes to the MAE above. First, it makes positional embeddings learnable and concatenates these to the input rather than adding them. This is to give the model more flexibility in assigning position embeddings to each pixel, so that it can learn spatial relationships between pixels. Second, instead of using self-attention in the decoder, we use cross-attention with the learned embeddings from the encoder. Intuitively, this means that the masked pixels (i.e. unsampled locations) will not calculate attention with each other, but will only calculate attention with the *sampled locations* learned on the encoder side (plus a single classifier token). Practically, it means that key and value vectors for cross-attention only have to be calculated for the sampled pixels, while query vectors only have to be calculated for the unsampled pixels. With a small number of sampled pixels, this reduces GPU memory requirements significantly and allows for faster processing of larger batches. Finally, when sampling the complete map, we take building locations into account so that no samples are drawn from locations occupied by buildings. We also allow the MAE to take as input a pre-sampled map with a mask indicating sampled locations.

One limitation of the MAE is that it expects all images / maps within a batch to have the same number of sampled pixels. This is possible when sampling is done within the MAE itself, because we can just set the number of samples per batch to a specific number. However, with pre-sampled maps, we can't enforce this unless we bin all maps with the same number of sampled pixels together or set batch size equal to 1. It might be possible to implement the first option with a dataset in the future, or to change the encoder so that it can accept variable-sized inputs within a single batch, but for now we have simply restricted all models to only accept a batch size of 1 when operating on pre-sampled maps.