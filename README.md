# Two-Stage Radio Map Estimation

This repository builds on [previous work](https://github.com/GeoAICenter/radio-map-estimation-pimrc2023) in radio map estimation (RME), predicting radio power across a two-dimensional area. As in previous experiments, we don't have knowledge of transmitter location(s), and the environment contains buildings which block and reflect radio signals. We take as input a small number of radio power measurements and a mask showing the locations of these sampled measurements and the buildings in the environment. The model then outputs a complete radio map showing predicted radio power at all locations in the 2D area (optionally including predictions at building locations).

![Image](images/example_input_output.png)

*Sample Input and Output: Sampled Map and Environment Mask are fed as input to the model, which outputs Complete Radio Map. Input and ground truth maps are adapted from the RadioMapSeer Dataset, discussed below.*

The idea of two-stage radio map estimation is to split the task of RME into two parts: first, taking the sparsely sampled measurements and predicting a dense radio map *without reference to buildings*, as if the signal were propagating through empty space; second, taking this dense radio map prediction and convolving it with a map of building locations ("Building Mask") to produce a more accurate radio map that takes the physical environment into account.

![Image](images/example_input_output_2.png)

*Sample Input and Output of Two-Stage RME: Instead of jumping straight from the sampled map to the complete radio map with buildings, an intermediate map is produced ("Dense Map") that ignores buildings and just extrapolates from sample strength and location.*

The intuition behind this is that radio propagation has unique physical characteristics, in particular a source of propagation (i.e. a transmitter) and a relationship between signal strength and distance from that source, that most image data don't have. A model that can learn these physical characteristics will presumably perform better than one that treats the radio map like a normal image. The first stage of the model is designed to learn these spatial relationships, and the second stage to combine this with environmental information. Our main model, MAE_CBAM, uses a masked vision transformer with learnable position encodings, self-attention, and cross-attention to accomplish the first goal, and a CNN / UNet with CBAM-style gated attention for the second. Other models replace / remove different elements of this main model to carry out ablation studies.

## Model Architectures

The two-stage RME models are composed of two sub-models, one for each stage. As stated above, for the main [MAE_CBAM](models/mae_cbam.py) model, these are a masked vision transformer and a UNet with CBAM gated attention. The masked vision transformer is adapted from the code for Masked Autoencoders Are Scalable Vision Learners ([paper](https://arxiv.org/abs/2111.06377) | [github](https://github.com/facebookresearch/mae)), and the UNet is adapted from our previous code with an added implemention of CBAM: Convolutional Block Attention Module ([paper](https://arxiv.org/abs/1807.06521) | [github](https://github.com/xmu-xiaoma666/External-Attention-pytorch/blob/master/README_EN.md#6-cbam-attention-usage)).

For ablation tests, we also have an [MAE_UNet](models/mae_unet.py) model, which removes CBAM attention from the UNet in the second stage of the model; the [Interpolation_CBAM](models/interpolation_cbam.py) and [Interpolation_UNet](models/interpolation_unet.py) models, which replace the masked vision transformer with basic interpolation; and the [CBAM](models/cbam.py) and [UNet](models/unet.py) models which remove the first stage altogether and feed the sparsely sampled map directly into a UNet with or without CBAM attention. The UNet model here is similar but not identical to the UNet model in our previous work.

*Diagram of all two-stage models (MAE_CBAM, MAE_UNet, Interpolation_CBAM, Interpolation_UNet). Single-stage models (CBAM, UNet) omit the first stage.*

**Important implementation note:** while the above diagram shows a sampled map being fed into the model as input, for training purposes all models actually taken in the *complete* radio map as input and carry out sampling within stage 1 of the model (so the model does not "see" the complete map when making its predictions, but carries out sampling as a pre-processing step within the model). For inference or evaluation, the user can set the hyperparameter *pre_sampled=True* to feed already sampled maps directly to the model, but (1) such maps have to have a batch size of 1, and (2) the sampled maps must have an accompanying mask that specifies sampled locations (if *pre_sampled=False*, the model generates this mask while sampling the complete map). The reason for the batch size of 1, and why training the models on pre-sampled maps isn't an option, is due to limitations in the training set, and could be changed in the future. This will be discussed in more detail in the sections on the MAE sub-model and the dataset below.

## Sub-Model Architectures

The sub-models that make up stages 1 and 2 of the two-stage models are saved in the sub_models folder, and are all preceded with an underscore "_" to distinguish them from similarly named full models. The four sub-models currently implemented are discussed below, with links to relevant papers and github repositories where they have been copied or adapted from other projects.

**MAE** ([paper](https://arxiv.org/abs/2111.06377) | [github](https://github.com/facebookresearch/mae))

The masked autoencoder (MAE) is a masked vision transformer with similar principles to masked language models, e.g. BERT. It takes a complete image, masks out parts of it, and then predicts the masked parts. Our implementation makes a few changes from the linked paper and code above. First, it makes positional embeddings learnable and concatenates these to the input rather than adding them. This is to give the model more flexibility in assigning position embeddings to each pixel, so that it can learn spatial relationships between them. Second, in the decoder that predicts the values of masked pixels, we use cross-attention between the masked and unmasked pixels (rather than concatenating masked and unmasked pixels together and using self-attention between all of them). Intuitively, this means that the masked pixels (i.e. unsampled locations) will not calculate attention with each other, but will only calculate attention with the *sampled locations*. Practically, it means that key and value vectors for cross-attention only have to be calculated for the sampled pixels, while query vectors only have to be calculated for the unsampled pixels. With small numbers of sampled pixels, this reduces GPU memory requirements significantly and allows for faster processing of larger batches. Finally, when sampling the complete map, we take building locations into account so that no samples are drawn from locations occupied by buildings. This allows the MAE to take complete maps as input and sample them itself, or it can be given pre-sampled maps as input (by setting *pre_sampled=True*).

One limitation of the MAE is that it expects all images / maps within a batch to have the same number of sampled pixels. This is possible when sampling is done within the MAE itself, because we can just set the number of samples per batch to a specific number. However, with pre-sampled maps, we can't enforce this unless we bin all maps with the same number of sampled pixels together or set batch size equal to 1. It might be possible to implement the first option with a dataset in the future, or to change the encoder so that it can accept variable-sized inputs within a single batch, but for now we have simply restricted all models to only accept pre-sampled maps when the batch size is 1.

**CBAM** ([paper](https://arxiv.org/abs/1807.06521) | [github](https://github.com/xmu-xiaoma666/External-Attention-pytorch/blob/master/README_EN.md#6-cbam-attention-usage))

The Convolutional Block Attention Module (CBAM) is a type of gated attention applied to feature maps in a CNN that squishes some features to 0 and allows others to pass through unchanged (more accurately, it multiplies all features by a factor between 0 and 1). The CBAM sub-model is a UNet with CBAM-style attention. The UNet has three "levels", where each level consists of three convolution + CBAM layers followed by downsampling or upsampling.

CBAM was the first type of attention we considered applying to RME, with the thought that it would learn to emphasize features in areas with more sampled measurements and downplay features in areas with fewer sampled measurements, while also potentially responding to implicitly learned features such as possible transmitter positions. However, since the input is already so sparse (all unsampled locations are filled with 0 by default), an attention mechanism that operates by squishing unimportant values to 0 might not have any affect here. This gave us the idea of filling in those unsampled locations and creating a "dense" map (i.e. stage 1 of the two-stage model). Our initial idea was to use interpolation to do this (realized in the Interpolation_CBAM model), though we actually started on the more powerful Masked Autoencoder (MAE_CBAM). We also created a CBAM model to allow the UNet with CBAM to operate directly on the sparse maps as an ablation study.

**UNet**

The UNet sub-model is identical to the CBAM sub-model above, but with the Convolutional Block Attention Module removed. It is used in the MAE_UNet, Interpolation_UNet, and UNet models for ablation studies to see whether the Convolutional Block Attention Module actually improves model performance in any of those cases.

**Interpolation** ([code](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.griddata.html))

To see whether the Masked Autoencoder (MAE) actually improves model performance, we replace it with nearest neighbor, linear, or cubic interpolation in the Interpolation_CBAM and Interpolation_UNet models. The implementation is taken from SciPy's interpolate.griddata function.

A practical challenge of using SciPy's interpolation function is that it requires putting all input tensors back onto the CPU and calculating the interpolated values for each map sequentially in a for loop. Because of this, models that use interpolation train much more slowly than those that use MAE, despite interpolation not having any learnable weights. If a function allowed multiple maps to be interpolated simultaneously on the GPU, this could speed up training significantly. The issue is raised in a PyTorch feature request [here](https://github.com/pytorch/pytorch/issues/1552), with multiple solutions proposed for different cases, but I am not sure if any would fit our specific case.